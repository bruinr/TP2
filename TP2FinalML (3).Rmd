---
title: "House Prices Kaggle Competition"
author: "Team 6 - Carrington Metts, Collin Parker, Bruin Richardson, Lauren Winstead"
date: "4/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# House Prices - Ames, Iowa { .tabset}

## Competition Overview {.tabset}


**Objective of Business Problem**

The objective of the business competition being analyzed is to utilize the explanatory variables provided describing all observed features of residential homes in Ames, Iowa to run an advanced regression model predicting the final price of each home. The output of our objective will be in the form of [ID, SalePrice] where ID is a unique identifier for each house (*row*) of our dataset.

<center>

![]($.jpg){width=20%} | ![](crib.jfif){width=20%}

</center>


**Origin of Data**

* The data provided comes from the **"Ames Housing Dataset"** in Ames, Iowa 

* The data was put together by *Dean De Cock* for the purpose of data science education, spanning from 2006 to 2010, and details the sale of residential property in Ames, Iowa

* Considered to be a more modern and expanded version of the *Boston dataset*, one we have used a number of times throughout our Machine Learning courses


**Key Features**

* Dataset contains **2930 oberservations** with 1460 provided in a train.csv file and 1459 provided in a test.csv file with columns detailing all considered features of homes

* 80 total variables -  23 nominal, 23 ordinal, 14 discrete, and 20 continuous explanatory variables

* Examples of explanatory variables included are: 

  + MSSubClass - Type of Dwelling (Duplex, 1-story 1946-present (new), 1-story pre-1946 (old), 1-story PUD (new), 2 family conversion, etc)
  
  + MSZoning - General Zoning Classification (Agriculture, Commercial, Residential-High Density, etc)
  
  + Neighborhood - Physical location in Ames city limits
  
  + YearRemodAdd - Remodel date (construction date if no remodeling done)
  
  + Heating - Type of heating (Floor Furnace, GasA (forced warm air furance), GasW (steam heat), Wall)
  
  + Exterior2nd - Exterior covering on house (Asbestos Shingles, Brick Face, Plywood, etc)
  
<center>
  
![Example Data](RowDataEx.png){height=100%}
<center>


## Existing Work - Summaries & Critiques {.tabset}

### Notebook #1 {.tabset}

#### Summary {.tabset}

**Data Import and Cleansing**


As with any data science experimentation, the first step in the process is to install the required packages. Here, the author installed the following: library(data.table), library(ggplot2), library(randomForest), library(dplyr), library(corrplot), library(knitr), library(kableExtra), library(easyGgplot2), library(caret).

From there, the author read in the training and test datasets provided by kaggle. The author then performed some exploratory analysis on the data in search of NA’s, and found that a number of features in the dataset contained Null or NA values. The author implemented five different methods to clean the data:  

* **Method 1** - Replacing missing values by “0”
 These numeric variables use “NA'' to represent the absence of what is being measured. For instance, if a house has a NA in the BsmtFullBath variable, it means it doesn’t have any full bathroom in the basement. Replacing a null value with zero is the intuitive imputation for this issue. 

* **Method 2** - Replacing missing values by “None”
Same as before, but with factor variables.

* **Method 3** - Replacing missing values by the most common value
As the most recurrent value in the factor variable, there is a higher probability that we are not making a mistake by assigning the most common value as the replacement to NAs. 

* **Method 4** - Replacing missing values by the median. 
Same as before, but now with numerical variables.

* **Method 5** - Replacing a null value for GarageYrBlt. 
The assumption is made that we can impute the year that the house was built in place of a null value for GarageYrBlt.
The final piece of data cleaning was to ensure that all of the variables are classified correctly. The author correctly identified that MSSubClass was incorrectly characterized as numeric, when it should be a factor.



**Feature Engineering**

Now that the data is clean, the author attempts to create new features that could help have a stronger predictive power than they otherwise would on their own. The author did not remove any of the old features from the dataset, however. The three features the author created are as follows: 

* **“Total Area” Feature** -
 Created by taking the sum of the basement area and the ground living area.
 
* **“Total Number of Baths” Feature** - Created by summing all bathroom features

* **“Area Above Ground” Feature** -  Created by summing the areas of the first and second floors

**Feature Selection**

Feature selection is broken down into two distinct sections: Numerical Variables and Factor Variables. 

* **Numerical Variables**

  + Here, the author subsets the training data so only numerical variables, such as square feet or acreage, are being analyzed.  Now the author can use this new table to check the correlation between numerical features. The goal is to select features that best predict “SalesPrice”. This table, that is the correlation of all the numerical features to the label “SalesPrice”, is then sorted in descending order. The author found that his new features performed better than the original ones. The final numerical features list consists of the following: OverallQual (A numerical ranking of overall quality of finish and materials), TotalArea, AreaAbvGround, GarageArea, TotalBaths, YearBuilt.


* **Factor Variables**

  + The same subsetting process is used to analyze the training data so only features that are factors are being considered. The author then runs a simple random forest model with all of the subsetted factors as features, and creates an importance table from the results of the random forest model. The author chooses to take the top seven features to use in the model. In order of most important to least, they are: Neighborhood, MSSubClass (the building class), FireplaceQu (quality of the fireplace), KitchenQual, ExterQual (exterior material quality), BsmtQual (height of the basement), HouseStyle.

**Label Analysis**

Now that the numerical and factor variable features have been selected, the author turns to the label, “SalesPrice” to conduct further analysis. Unsurprisingly, the label data is positively skewed, as housing prices will not fall below zero, with outliers that represent multiples of the mean. 	The author also determined that there was heteroskedasticity present in the label, so a log transformation was applied to the label.

**Exploratory Data Analysis**

*Numerical Variables*

To check if the log transformation was successful, the author plots each of the features with the log of sales price. The linear relationship between each of the numerical features and the log of Sales Price indicates that the log transformation of the label increased the robustness of the model.

*Factor Variables*

The author points out the variance between sales prices in different neighborhoods, which is to be expected. 

**Final Model**

At this stage, the author creates his final model. He chooses to do a random forest model, and he includes the 13 features previously mentioned: 

* **Numerical** - OverallQual, TotalArea, AreaAbvGround, GarageArea, TotalBaths, YearBuilt

* **Factor** - Neighborhood, MSSubClass, FireplaceQu, KitchenQual, ExterQual, BsmtQual,  HouseStyle.

The author then fits this model, and predicts test labels using the test dataset. These predictions are written to a csv file, which are then uploaded to kaggle to test the model’s predictions against the actual test label values. In this case, the author’s best model achieved a RMSLE of 0.15019. 
Not sure how to interpret root mean squared log error








#### Critique {.tabset}

This Kaggle user’s approach to predicting housing prices offers insight into the importance of proper feature engineering, model selection, and cross-validation to minimize the target error metric. The submission illustrates questionable data cleaning and imputation techniques, incorrectly identifies significant features for the given model, and fails to utilize any sort of cross-validation methods.

Taking a closer look at the author’s initial data cleaning techniques, the author begins by replacing numerical NA’s with 0, and categorical NA’s with “None.” These are reasonable assumptions given the documentation, which indicates that a “0” or “None” is not a missing value, but a zero value (i.e. a house does not have a garage). The author also replaces actual missing categorical data with the most common value. However, the author’s data cleaning and imputation stops here. In this example, there was no consideration given to clear outliers in the data, present in features such as GrLivArea, LotArea, X1stFlrSF, and TotalBsmtSF.

 Further, there are variables that exist in this dataset that are represented as categorical, yet clearly represent a ranked hierarchy. For example, ExterQual, BsmtQual, and KitchenQual are represented as No, Po, Fa, TA, Gd, or Ex. The author of this notebook chose to not modify these features, however, we would suggest converting these categorical variables into numerical and then creating a new binary variable that would be able to reward “good” quality and penalize “bad” quality, as represented by the 6 different ranks.  There are other features in this dataset that would be better suited to the model as binary. For instance, features like LotShape, LandContour, LandSlope, PavedDrive, and Electrical represent only two options, and should be binarized to further simplify classification. Other features exist that appear to be numerical, yet are actually categorical. MSSubClass, MoSold, and YrSold should be changed to factors. The author successfully converted MSSubClass to factor, yet failed to alter MoSold and YrSold. These should be represented as categorical as they represent periods of seasonality and do not necessarily have a linear relationship with sale price.
 
Interestingly, the author of this notebook chose to determine significant numerical features without creating a model. First, the author focused on determining significant numerical features, which was done by creating a correlation plot of all numerical features to the Log of Sale Price.  This makes intuitive sense, perhaps, but the author is analyzing the impact of these features through a narrow scope. Irrespective of the categorical features, this method of feature selection has some merit, but it is bizarre to ignore an entire subset of features (i.e. the categorical features) that could materially change the significance of numerical features relative to the target. The author keeps the top 6 numerical features by correlation in the final model.
A similar process is undertaken to select categorical features. The author creates an importance table of only the categorical features relative to the target Sale Price. Again, this separation of features ignores the fact that the features interact with each other in the regression analysis, and should be avoided as a means of selecting features. 

The lack of cross-validation in this submission is likely a contributing factor to this notebook’s lower than average RMSLE. The author makes no attempt to train the model using any algorithms besides random forest. There is no specific nuance to this dataset that makes it a perfect fit to a random forest model, so it is odd that the author did not experiment with other models such as lasso, or xgboost. Further, there was no hyperparameter tuning within the random forest model itself. Methods like K-Fold CV and RandomizedSearchCV could have been implemented to tune the random forest model and decrease RMSLE.

When comparing this notebook to the reproducibility checklist, there are clear instances where the author could improve commenting, or better explain methodology.  For instance, when the author Log transforms the target and plots the categorical features, a comment is made that “we can already see the impacts of log transformations on the dataset” yet no effort is made to explain these transformations to the reader. Further making things confusing for the reader is the lack of explanation as to why the author implemented a random forest model. With no cross-validation attempts to benchmark one algorithm against another, and with no verbal explanation as to the advantages of a random forest compared to other algorithms, the reader is rightfully skeptical that a random forest is the optimal algorithm for this dataset. 


### Notebook #2 {.tabset}

#### Summary {.tabset}

**Housing Price Predictions Using Advanced Regression Techniques Overview**

This script predicts housing prices using the following advanced regression techniques: ridge regression, lasso regression, random forest regression, and linear regression. The user created these different models, verified the accuracy of the model, and wrote  predictions on the models.

**Techniques Implementation: Initial Steps and Data Preparation**

The user began by:
1. imported  the data and set up the training and test data
2. analyzed this data to determine how to deal with missing data.  
3. Set the seed for reproducibility. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. 
4. inputted the missing values using the missforest package which runs a randomForest on each variable and used the observed part to predict  any missing  values. 
5. verified the skewness of the dataset features and took the log transformation of the features for which skewness is more than 0.75. 
6. used the dummervars() function in the caret package for one-hot encoding of categorical variables. 


**Ridge and Lasso Summary**
The user similarly created Ridge and Lasso models using the repeatedcv() function and set the number to 10 with 10 repeats
Setting the method to repeated cv() controls the type of resampling and is used to specify repeated k-fold cross-validation. 



**Ridge Model In Depth Analysis**

1. created the model using the trainControl() function and set the method to “repeatedcv”, number to 10, and repeats to 10
2. created a lambas variable using the seq() function.
3. created the ridge model using glmnet, set the metric to RMSE, and expanded the grid by setting alpha to 0 and lambda to the lambas variable that was recently created. 
4. iterated through a sequence of increasing lambdas that were declared in the lambdas variable to utilize the mode for further interpretation of finding the optimal lambda. Note: The best lambda, however, is not disclosed or even implemented. 


Next, the user attempted to verify the accuracy of the ridge model by calling the results variable; this shows error metrics for the variables of the model. This was meant to help the user pick the best lambda based on the optimal RMSE error metric; however, the user did not disclose his finding on the given output. 

Sample of the Ridge Model error metric output:


![10.2 Verify Accuracy](10.2VerifyAccuracy.png){width=40%}

install.packages("psyche")

 The user then looked at the contribution of each variable to make a prediction to show glmnet variable importance.
 
 

![10.3 Variable Contribution](10.3TakeALook.png){width=40%}


Finally, the user makes predictions on the test set of the model to file for kaggle. 


![Ridge Prediction Sample Output](RidgePredsSampleOutput.png){width=25%}

**Lasso Regression Model In Depth Analysis**

The Lasso Regression Model is then utilized for analysis on the missing data. The user must first create the lasso model by predicting the sale price on the dataset using the train() function. This is implemented through the glmnet() method and by setting the metric to “RMSE.” The user also uses the expand.grid() function by setting alpha to 1 and setting up a lambda grid. The difference between ridge and lasso regression is that lasso tends to change coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero. The user incorrectly implements this expand.grid() function which is meant to help identify the best lambda, which he never does. This will be elaborated in the critique.

The user verifies the accuracy of the model by calling the results variable in the lasso model. The intention is to assess RMSE as the error metric, which seems to stay the same as lambda increases. This shows that the lasso model is not efficient in reducing error metric on the data set. This will be elaborated in the critique. Below is a sample of the accuracy assessing the error metrics of the model, which we see of the Lasso Model:

**Lasso Error Metrics Sample Output**


![Lasso Error Metrics Sample Output](LassoErrorSampleOutput.png){width=60%}


Next, the user analyzes the contribution of each variable to make a prediction using the varImp() function. Through this, we can see the following top 20 variables shown out of 289 in descending order. 



![Lasso Model Variable Importance Sample Outputt](LassoVImportanceSample.png){width=25%}


The next step is to make predictions of the test dataset.

![Lasso Model Housing Price Predictions Sample Output](LassoHousingPredSample.png){width=30%}

**Ridge and Lasso Results Synopsis**

The user then assessed the accuracy of the models by setting the metric to “RMSE, the user called $results of the given model
with intention of assessing this information to help pick the best lambda based on the optimal RMSE error metric;
however, the user did not disclose his finding on the given outputs.

Example Output: 

![10.2 Verify Accuracy](10.2VerifyAccuracy.png){width=40%}

In the lambda model when the user verifies the accuracy, the user’s  assessment of RMSE error metric shows to stay the same as lambda
increases. This shows that the lasso model is not efficient in reducing error metric on the data set. 

![Lasso Error Metrics Sample Output](LassoErrorSampleOutput.png){width=60%}

**Random Forest Regression Model**

The third model is a random forest regression model. The user creates the model by:
1. predicting the sale price based on the other variables
2. setting the method to “RMSE,” meaning the user focused on RMSE as the error metric for this model with the intention of reducing it. With this function, the user also sets trControl() function’s method to “repeatedcv,” 
3. The user also  sets tuneGrid to expand.Grid(), where the user implements mtry to c(5) and sets importance,
allowParallel, and prox to True.


**Random Regression Code**


![Random Regression Code](RandomRegressionCode.png){width=50%}


The user then verifies the accuracy of the model by calling the results variable from the model and 
analyzing the standard deviation of the error metrics, specifically the RMSE. 



![11.2 Verify Accuracy](11.2VerifyAccuracy.png){width=50%}

The user then analyzed the contribution of each variable within the model by using the varImp() function to 
assess the importance of each variable. 

Notice how the importance values also do not say anything about the relationship between the predictor and the outcome, 


The output with 20 variables in descending order of contribution level:


![20 Variables, desc contribution level](20VarRF.png){width=25%}

Finally, the user makes a prediction on the test set using the predict() function.


**Random Forest Prediction Sample Output**


![Random FOrest Prediction Sample Output](RFPredSample.png){width=30%}


**Linear Regression Model**

1. created a model and sets the method to “lm” and metric to “RMSE”as the error metric


![Linear Regression Model Code](LRCode.png){width=60%}


2. verified the accuracy of the model by calling the results variable from the model in order to assess the model’s error metrics. 


Linear Regression Accuracy Sample Output

```{r, echo=FALSE}
labels <- c("intercept", "RMSE", "Rsquared", "MAE", "RMSESD", "RSquaredSD", "MAESD")
values <- c("TRUE", "32588.26", "0.8393711", "19637.44", "9691.06", "0.08381256", "2300.314")


table <- setNames(labels, values) ; table



```
  
The user next analyzes the contribution of each variable to make a prediction using the varImp() function on the linear model. Next the user is able to make predictions based on the test set and rounds this prediction to two decimal places. 

![Linear Model Predictions](LMPreds.png){width=30%}




























#### Critique {.tabset}
Overall, the user’s approach with regression techniques pertaining to Housing Price prediction does a broad and weak implementation of 
1. data pre-processing
2. cross-validation technique repeatedcv() 
3. use of the RMSE error metric within his/her models. 
4. does not implement the best lambda, and only prints the results to attempt at analyzing the accuracy of the model that 
implements the training data rather than running through data. 


**Data Preprocessing and Feature Engineering**


First, the user has a poor strategy for improving the quality of the data by assessing the skewness before ever attempting to improve the quality of the data. Assessing the skewness should be done __after__ data preprocessing.


The user initially separated  the numeric and character values in the training data which he only then assessed the missing values within the numeric data, using the dummyvars() function that predicted NA values in the numeric category. 

By separating the numeric and character attributes into separate vectors, the user created models that do not analyze attributes with characters at all. This implementation created inaccurate data, where more specific or intuitive approaches could have been utilized.  

First, this can be fixed by including “StringsasFactors=TRUE'' when pulling in the data. 


The next step in improving the data is to fill in the values that can be done __manually__ rather than implementing statistics to predict NA values in solely the numeric data vector to be utilized. 

Not sure if the user actually looked at the dataset based on the lack of data preprocessing and feature engineering. 
1. Need to include attributes with characters, which is a vast portion of significant variables needed for the regression model/models.
2. There are  numeric values in certain records that can manually be replaced with a “0,” such as the attributes “LotFrontage”, “MasVnrArea”, and “BsmtFinSF1”. The function for implementing this looks like this:
```{r, results=FALSE}
 #dataset$BsmtFinSF2[is.na(dataset$BsmtFinSF2)] <- 0

```


3. changing the values of NA’s that have actual meanings, no meanings at all, and others that have a more intuitive approach:

- the NA records in attributes with actual meanings such as in the Alley attribute can be assumed to actually mean “No,” with no meaning at all such as in the KitchenQual attribute can be replaced with the most common value of this feature, and with a more intuitive approach such as for the feature GrgYrBlt, we can assume that the year the garage was built is either the year the house was built or if the house was remodeled we could take the average of both years, given a 50/50 probability that it could be either/or. Another way to improve the quality of the data is to change empty values to “No.” By this point, it can be worth assessing the NA values in the dataset. 
  
- could identify outliers in the dataset by running a linear regression to predict the sale price based on the predictors in the dataset. Once this is done, the user could plot the linear regression model to assess the cook’s distance plot for outliers to be removed. He/she can also plot these features against the target variable to individually assess the predictor variables. The user could either remove the outliers or transform them to the mean of each variable. 

- assess character variables into continuous numerical variables. Some character variables require a transformation to ranking/ binary values, and others require completely new features. For example, the ExterQual variable requires a ranking and binary transformation. Also, new features that could be added could improve data quality. For example, the user could create features to determine whether the house was remodeled after it was sold --if so, alter the data. The user could further create features to analyze whether the house was remodeled after it was sold, to assess the number of years since the house was remodeled and sold to disclose how old the house is the time since it was last sold, to calculate the total surface area of the house. Furthermore, the user should create a calculated field to determine hot months for homes to be sold as well as calculate the attributing month the individual house was sold in with a given value. Some character features solely require binarizing, because they are primarily unique. For instance, LotShape would be binarized, because it either has a regular shape or it does not. The user could also binarize area-related features.

**Model Errors**

1. Needed to obtain the lasso or ridge regression fit with:
```{r, results=FALSE}
 #cvfit <- cv.glmnet(X, y)
```

2. Needed to get the best lambda value of lambda out
```{r, results=FALSE}
 #lambda.min <- cvfit$lambda.min

```

3. Could even even asses the best mse 
```{r, results=FALSE}
 # mse <- cvfit$cvm[cvfit$lambda == lambda.min]

```
	
4. User’s error metric assessment.  The user runs tests and produces an output showing multiple items.  However, they don’t say which items in this output are the measure that they’re focusing on.  It can be inferred that RMSE is likely the error measure of choice, but the user does not specify which model produces the best error measure for accuracy analysis.  This is a critical area for improvement since it makes it difficult to assess the user’s results.


## Our Approach - XGBoost {.tabset}

### Coded Solution {.tabset}

#### Setup {.tabset}
As always, we'll begin by setting up our environment. We'll need several packages:
dplyr for data cleansing, caret for cross-validation, and xgboost and plyr for 
creating the XGBoost model. 
``` {r libraries, message=FALSE, warning=FALSE}
rm(list=ls())
library(dplyr)
library(caret)
library(xgboost)
library(plyr)
```

We'll then read in the test and train data from CSVs. 
``` {r read_data}
train <- read.csv("train.csv", header=T, stringsAsFactors=TRUE)
test <- read.csv('test.csv', header=T, stringsAsFactors=TRUE)
```

The first step of data analysis involves correcting missing values. Therefore, 
we'll merge the test and training sets, so we can perform the same operations on both. 
The training set is unlabeled, so we'll enter its SalePrice as NA. 
We'll also add a Set column so we can easily separate the test and training observations.

```{r combine_sets}
test$SalePrice <- NA
test$Set <- 'Test'
train$Set <- 'Train'
full_data <- rbind(test, train)
```

#### Data Cleansing {.tabset}

##### Numeric to Factor

First, we need to convert a few variables from int to factor. MSSubClass is 
represented as an integer, but its values describe the type of home (1-story, 
duplex, etc.). The month and year of the sale should also be factors. 

```{r to_factor}
factorVars <- c('MSSubClass', 'MoSold', 'YrSold')
#use lapply to apply the factor() function to each column 
full_data[, factorVars] <- lapply(full_data[, factorVars], factor)
```

We'll then obtain the names of all numeric and factor variables to use in later 
steps. 
``` {r get_var_names}
numericVars <- which(sapply(full_data, is.numeric))
numericVars <- names(numericVars)
numericVars <- numericVars[-1] #remove the ID name: we'll be deleting that variable
factorVars <- which(sapply(full_data, is.factor))
factorVars <- names(factorVars)
```

##### Remove Missing Values
Many numeric variables have missing values that should be replaced with 0. For 
example, if a house does not have a garage, the garage square footage
should be set to 0. We'll use the apply() function to replace the relevant values.

```{r numeric_zero}
#set select missing numeric values to 0
zeros_numeric <- c("LotFrontage", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", 
                  "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", 
                  'GarageYrBlt', 'GarageCars', 'GarageArea')
full_data[,zeros_numeric] <- apply(full_data[,zeros_numeric], 2, 
                    function(x) {
                      replace(x, is.na(x), 0)
                    } )
```

Similarly, categorical variables should be set to 'None' if the house does not 
have the relevant feature (garage, pool, etc.)
```{r categorical_none}
#set select missing categorical values to 'None' 
cat_vars <- c("Alley", "BsmtQual", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", 
              "FireplaceQu", "PoolQC", "Fence", "MiscFeature", "GarageType", "GarageFinish", 
              "GarageQual", "GarageCond", "BsmtCond", 'MasVnrType')
full_data[,cat_vars] <- apply(full_data[,cat_vars], 2, 
                    function(x) {
                      replace(x, is.na(x), "None")
                    })

#reconvert these categorical variables to factors 
full_data[, cat_vars] <- lapply(full_data[, cat_vars], factor)
```

Other categorical values are missing but should not be 'None'. For example, 
every house should have some functionality score, which indicates the overall 
condition of the house. Most of these variables are overwhelmingly skewed towards
one class. Therefore, we chose to replace missing values 
with the most common value. 

```{r replace_common}
#replace missing categorical values with the most common value 
replace_common <- c('Functional', 'MSZoning', 'Utilities', 'Exterior1st', 
                    'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional', 
                    'SaleType')

for (var in replace_common) {
  full_data[var][is.na(full_data[var])] <- names(sort(-table(full_data[var])))[1]
}
```
At the end of this step, we can see that there are no NA values remaining in 
any of the features. 
```{r count_na}
sum(is.na(select(full_data, -c(SalePrice))))
```

##### Address Ordinal Variables 
Many of the categorical variables in this dataset are actually ordinal measures 
of quality. For example, the basement quality is represented with 
one of 6 ratings from 'None' to 'Excellent'. These variables should actually be 
encoded as numbers, since houses with high scores will be more desirable than 
houses with low scores.

To apply the transformation, we'll create a vector that matches each factor
to the corresponding number. We'll then use the revalue() function to replace 
each factor with the number. Since many of the factors are encoded differently, 
we'll have to create multiple different conversion vectors. 

```{r ordinal_to_num, warning=FALSE, message=FALSE}
#Convert quality and condition variables to numeric 
ordinalVars <- c('ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC',
                  'KitchenQual', 'GarageQual', 'GarageCond', 'PoolQC', 'FireplaceQu')
qualities <- c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
full_data[, ordinalVars] <- lapply(full_data[, ordinalVars], function(x) {
                                                                as.integer(revalue(x, qualities)) } )

#Same thing for basement exposure 
bsmt_exp_qual <- c('None'=0, 'No'=1, 'Mn'=2, 'Av'=3, 'Gd'=4)
full_data$BsmtExposure <- as.integer(revalue(full_data$BsmtExposure, bsmt_exp_qual))

#Same thing for basement finish type 1 and 2
bsmt_fin_type <- c('None' = 0, 'Unf'=1, 'LwQ'=2, 'Rec'=3, 'BLQ'=4, 'ALQ'=5, 'GLQ'=6)
full_data$BsmtFinType1 <- as.integer(revalue(full_data$BsmtFinType1, bsmt_fin_type))
full_data$BsmtFinType2 <- as.integer(revalue(full_data$BsmtFinType2, bsmt_fin_type))

#Same thing for garage finish
garage_fin <- c('None' = 0, 'Unf'=1, 'RFn'=2, 'Fin'=3)
full_data$GarageFinish <- as.integer(revalue(full_data$GarageFinish, garage_fin))

#Same thing for functionality 
func_ratings <- c('Sal' = 0, 'Sev'=1, 'Maj2'=2, 'Maj1'=3, 'Mod'=4, 
                           'Min2'=5, 'Min1'=6, 'Typ'=7)
full_data$Functional <- as.integer(revalue(full_data$Functional, func_ratings))

#Same thing for PavedDrive
paved <- c('N' = 0, 'P'=1, 'Y'=2)
full_data$PavedDrive <- as.integer(revalue(full_data$PavedDrive, paved))

#Update list of ordinal variables 
ordinalVars <- c(ordinalVars, 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 
                 'GarageFinish', 'Functional', 'PavedDrive')

#Remove ordinal variables from list of factor variables 
factorVars <- setdiff(factorVars, ordinalVars)
```


#### Data Investigation {.tabset}
After cleaning the data, we still have our original 80 features. Of these, 34 are numeric, 
30 are categorical, and 16 are ordinal. Before creating the model, we'll perform 
some feature selection and transformations for some of the data. These operations 
should be determined based on the data in the training set; therefore, we'll begin 
by splitting the full dataset back into the training and testing sets. We'll 
also remove the house ID and the label indicating whether the observation belongs in the testing or training set, 
along with the dummy SalePrice entries from the testing set. 

```{r split_data}
train <- full_data %>% filter(Set=='Train')
train <- select(train, -c(Set, Id))
test <- full_data %>% filter(Set=='Test')
test <- select(test, -c(Set, Id, SalePrice))
```


##### Feature Selection
Within this dataset, many of the numeric variables have high correlation. To 
handle this, we will remove some highly correlated numeric variables. 

To investigate We can look at a subset of the full correlation matrix. From this chunk, we can 
see that YearBuilt and OverallQual are highly correlated. 
``` {r corr_matrix}
all_numVar <- train[, numericVars] #select all numeric variables 
full_cor <- cor(train[, numericVars], use="pairwise.complete.obs") #make correlation matrix 
full_cor <- as.matrix(full_cor)
full_cor[1:5, 1:5]
```
We can determine the pair of variables that have the highest correlation. 

```{r find_largest_corr}
all_numVar <- subset(all_numVar)
corMatrix <- cor(all_numVar, use="pairwise.complete.obs")
#set diagonal elements equal to 0
corMatrix[corMatrix == 1] <- 0
#print largest correlation value 
print(max(corMatrix))
#print variable names with largest correlation 
which(corMatrix == max(corMatrix), arr.ind = TRUE)
```
GarageArea and GarageCars have a correlation of 0.88. This intuitively makes sense,
as larger garages would fit more cars. From examining the full correlation matrix, 
GarageCars has a lower correlation with SalePrice. Therefore, we will remove it from 
the dataset. We'll then rebuild the correlation matrix with the subsetted dataset, 
find the new pair of variables with the highest correlation, and remove one. We'll 
repeat this until the highest correlation is less than 0.5. 

At the end of this process, 10 numeric variables were removed. These features will 
be removed from the training and testing sets.
``` {r remove_numeric}
VarsToRemove <- c('GarageCars', 'TotRmsAbvGrd', 'X1stFlrSF', 
                  'X2ndFlrSF', 'BsmtFinSF1', 'FullBath', 'GrLivArea', 
                  'OverallQual', 'YearRemodAdd', 'GarageYrBlt')
train <- train[,!(names(train) %in% VarsToRemove)]
test <- test[,!(names(test) %in% VarsToRemove)]
numericVars <- setdiff(numericVars, VarsToRemove)

```

##### Data Transformations 
The next step is to center and scale the numeric data and take log transformations 
of the variables that are highly skewed. This can be accomplished with one for loop.

```{r log_center_scale}
for (var in setdiff(numericVars, 'SalePrice')) {
    #take a log transformation if the skewness is greater than 1
    if (abs(psych::skew(train[var])) > 1) {
      train[var] <- log(train[var]+1)
      test[var] <- log(test[var]+1)
    }
  
    #center and scale all data except the response variable 
    train[var] <- scale(train[var])
    test[var] <- scale(test[var])
    
}

```

XGBoost requires that all categorical variables be transformed using one-hot encoding.
In R, this is done with the model.matrix() function. 
``` {r one-hot}
train_factors <- train[, (names(train) %in% factorVars)]
train_encoded <- as.data.frame(model.matrix(~., data=train_factors))
test_factors <- test[, (names(test) %in% factorVars)]
test_encoded <- as.data.frame(model.matrix(~., data=test_factors))
```

The ordinal variables do not require any transformation. Therefore, we'll recombine 
the numeric, categorical, and ordinal variables into a complete dataset. 

```{r recombine}
full_train <- cbind(train_encoded, train[,(names(train) %in% numericVars)], 
               train[,(names(train) %in% ordinalVars)])
full_test <- cbind(test_encoded, test[, (names(test) %in% numericVars)], 
                   test[,(names(test) %in% ordinalVars)])
```

Finally, we'll split the training set to create a validation set. Since the test 
set is unlabeled, this will allow us to objectively evaluate our model's performance. 

``` {r val_set}
trainIndex <- createDataPartition(train$SalePrice, p = .8, 
                                  list = FALSE)
train <- full_train[trainIndex, ]
validation <- full_train[-trainIndex, ]
```

#### Model Creation

Finally, the data is ready to be used in the XGBoost model. To find the best model,
we will use the 'caret' package to perform grid search with different combinations 
of hyperparameters. For the boosting tree, we will optimize three parameters: 

* **eta** is the model learning rate. It will range from 0.1 to 1. 
* **max.depth** is the maximum depth of each tree in the model. It will range from 1 to 6.
* **gamma** is the minimum reduction in loss that is required for the model to decide to split a tree node. 
It will range from 0 to 0.2. 

Once these ranges are defined, we can use caret to create a parameter grid. We'll 
also set the parameters for the grid search and cross-validation. In this case, 
the boosting algorithm will run for 100 iterations. Each model will be passed 
through 5-fold cross validation. Since there are multiple models, this portion 
takes approximately 10 minutes to run on a laptop. 

```{r tree_model, warning=FALSE, message=FALSE, results=FALSE}
eta <- seq(0.1, 1, by=0.1) #learning rate 
max.depth <- seq(1, 6, by=1) #depth of the tree 
gamma <- seq(0, 0.2, by=0.05) #minimum loss reduction


treeControl <- trainControl(method='cv', number=5, search='grid')
treeGrid <- expand.grid(nrounds=100, max_depth=max.depth, eta=eta, gamma=gamma,
                        colsample_bytree=1, min_child_weight=1, subsample=1)
treeTrain <- train(SalePrice~., data=train, method='xgbTree', trControl=treeControl,
                   tuneGrid=treeGrid)
model_params <- treeTrain$bestTune
```
Once the cross-validation has run, we can extract the optimal values of the 
parameters and create a model. 
```{r}
bestEta <- model_params$eta
bestGamma <- model_params$gamma
bestDepth <- model_params$max_depth
model <- xgboost(data = as.matrix(select(train, -c(SalePrice))), label=train$SalePrice,
                 nrounds = 100, max.depth = bestDepth, gamma=bestGamma, 
                 eta = bestEta, nthread = 2,  
                 objective='reg:squarederror', verbose = 0)
```

Finally, we can use the model to make predictions on the validation dataset and assess
the overall performance. 

```{r validate}
validation_preds <- predict(model, as.matrix(select(validation, -c(SalePrice))))
ModelMetrics::mae(validation_preds, validation$SalePrice)
ModelMetrics::rmse(validation_preds, validation$SalePrice)
```
The final RMSE is 0.1293, which is significantly better than the result obtained 
from the other notebooks we analyzed. The MAE is 0.0956. Since the response variable was 
log transformed, this means that, on average, the model's predictions differ from 
the actual sale prices by a factor of 1.1 (e^0.0956). 

The final step is to use the model to make predictions on the test dataset. 

```{r predict_test}
test_preds <- predict(model, as.matrix(full_test))
#undo results of log transformation 
test_preds <- exp(test_preds)
```
When uploaded to Kaggle, this result ranks at number 5399 of 8405. This could 
possibly be improved by removing outliers, tuning more hyperparameters, or 
removing even more features from the model. 







### Reproducability {.tabset}


According to the machine learning reproducibility checklist, our solution meets 
the criteria. While we did make some assumptions when deciding how to address 
NA values, all of them are justified according to the entries in the dataset. 

For the dataset, we described the elements of the data. We also provided a comprehensive
explanation and justification for all preprocessing steps, including the process
of creating a training/validation split. The data may be found at 
https://www.kaggle.com/c/house-prices-advanced-regression-techniques. 

This code is designed to be run independently. All required libraries are 
listed and loaded in the beginning. 

Finally, for our results, we included a detailed explanation of how to perform 
cross-validation and select the best hyperparameters. We also reported two metrics
for assessing the model performance. 





